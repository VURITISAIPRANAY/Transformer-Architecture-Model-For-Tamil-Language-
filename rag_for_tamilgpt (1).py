# -*- coding: utf-8 -*-
"""RAG_for_tamilGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5n1AtqzxjTbxe38tqOKTgVBXZdkYvFH
"""

!pip install git+https://github.com/huggingface/transformers -q peft  accelerate bitsandbytes safetensors sentencepiece

!pip install pypdf
!pip install langchain langchain_community huggingface_hub docarray sentence-transformers

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

model_name = 'abhinand/tamil-llama-7b-instruct-v0.1'

def load_quantized_model(model_name: str):

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        quantization_config=bnb_config
    )

    return model

model = load_quantized_model(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


# embeddings = HuggingFaceEmbeddings(
#     model_name=model_name
# )

!pip install -U langchain-huggingface

from google.colab import drive
drive.mount('/content/drive')

from langchain_community.llms import HuggingFaceHub
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from operator import itemgetter

import pypdf

from langchain_huggingface import HuggingFaceEndpoint

from langchain_huggingface import HuggingFaceEndpoint

def initialize_tokenizer(model_name: str):

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.bos_token_id = 1
    return tokenizer

tokenizer = initialize_tokenizer(model_name)

# llm = HuggingFaceHub(
#     repo_id=model_name,
#     model_kwargs={"temperature":0.3, "max_new_tokens":512},
#     tokenizer=tokenizer
# )

inputs ='வணக்கம் நீங்கள் யார்'
text = f"### Instruction:\n{inputs}\n\n### Response:\n"
input_ids = tokenizer(text, return_tensors="pt").input_ids.to(device)
op=model.generate(input_ids, max_new_tokens=512)
print(tokenizer.decode(op[0], skip_special_tokens=True))

loader = PyPDFLoader("/content/hdfc_corporate_tnc.pdf")
pages = loader.load_and_split()

parser= StrOutputParser()

'''
chain = model | parser
chain.invoke(input_ids)'''
# parser is used to parse the output of the model. meaning remove the "AI output": tag

from langchain.prompts import PromptTemplate

system_prompt='answer the instruction based on the input below'
input = "your name is tamilGPT"
instruction="what is your name"
template = """{system_prompt}
 ### Instruction: {instruction}
 ### Input: {input}
 ### Response:"""

# prompt = PromptTemplate.from_template(template)
# print(prompt.format(input="your name is tamilGPT", instruction="what is your name"))

# chain = prompt | model | parser
# chain.input_schema.schema() #to find the attributes that chain invoke expects
# chain.invoke({"input":"your name is tamilGPT", "instruction":"what is your name"})

prompt = PromptTemplate.from_template(template)

# !pip install docarray
# !pip install pydantic
# # ==1.10.8

from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_core.documents import Document

vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)

# vectorstore.as_retriever().invoke("what is the capital of france") #top_k = 3 for top three relevant pages, but it's not working. need to check the documentation



# chain = (
#     {
#         "system_prompt" : system_prompt,
#         "input": itemgetter("question") | vectorstore.as_retriever(search_kwargs={"k": 3}),
#         "instruction": itemgetter("question"),
#     }
#     | prompt
#     | model
#     | parser

# )

# chain.invoke({"question": "what is the capital of france"})


# for token in model.stream({question:"what is the capital of france"}):
#     print(token, end="",flush=True)

credit = vectorstore.as_retriever(search_kwargs={"k": 3}).invoke("credit card")

for doc in credit:
    print(doc.page_content)

def run_pipeline(question):

    docs = vectorstore.as_retriever(search_kwargs={"k": 2}).invoke(question)
    combined_text = "\n".join(doc.page_content for doc in docs)

    prompt_input = prompt.format(
        system_prompt=system_prompt,
        input=combined_text,  # Passing retrieved documents as input
        instruction=question
    )

    input_ids = tokenizer(prompt_input, return_tensors="pt").input_ids.to(device)

    output = model.generate(input_ids)

    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
    final_answer = parser.parse(decoded_output)

    return final_answer


question = "what is company mentioned in the pdf"
answer = run_pipeline(question)
print(question)
print(answer.split("### Response:")[-1].strip() if "### Response:" in answer else answer)

#print(answer) #for complete output

import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")